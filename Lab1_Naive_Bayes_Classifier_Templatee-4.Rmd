---
editor_options:
  markdown:
    wrap: 72
---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work break

-   *Name1 Surname1*:
-   *Name2 Surname2*:
-   *Name3 Surname3*:

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data\

**See [*this
link*](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)
for more detailed explanations & examples :) Also you can watch [*this
video*](https://youtu.be/O2L2Uv9pdDA?si=-ohkHVDuu3sLLGMq) for more
examples!**

## Data description

There are 5 datasets uploaded on the cms (data.zip)

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifierʼs possibilities on the unseen data.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(tm)
```

## Outline of the work

1.  **Data pre-processing** (includes removing punctuation marks and
    stop words, representing each message as a bag-of-words)
2.  **Data visualization** (it's time to plot your data!)
3.  **Classifier implementation** (using the training set, calculate all
    the conditional probabilities in formula (1))
4.  **Measurements of effectiveness of your classifier** (use the
    results from the previous step to predict classes for messages in
    the testing set and measure the accuracy, precision and recall, F1
    score metric etc)
5.  **Conclusions**

*!! do not forget to submit both the (compiled) Rmd source file and the
.html output !!*

## Data pre-processing

-   Read the *.csv* data files.
-   Сlear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
stop_words <- read.csv(file = "stop_words.txt", stringsAsFactors = FALSE)

test_path <- "data/test.csv"
train_path <- "data/train.csv"

get_clean_data = function(path, stop_words) {
  data <- read.csv(file = path, stringsAsFactors = FALSE)
  
  data$Message <- tolower(data$Message)
  data$Message <- removeNumbers(data$Message)
  data$Message <- removePunctuation(data$Message)
  data$Message <- stripWhitespace(data$Message)
  data$Message <- removeWords(data$Message, stop_words)
  
  return(data)
}
```

```{r}

train <- get_clean_data(train_path, stop_words$a)
test <- get_clean_data(test_path, stop_words$a)


tidy_text <- unnest_tokens(train, 'splitted', 'Message', token="words")

```

## Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

```{r}
most_frequent_spam <- tidy_text %>% filter(Category=="spam") %>% count(splitted, sort=TRUE)
most_frequent_spam <- top_n(most_frequent_spam, 30, n)

most_frequent_ham <- tidy_text %>% filter(Category=="ham") %>% count(splitted, sort=TRUE)
most_frequent_ham <- top_n(most_frequent_ham, 30, n)

ggplot( most_frequent_spam, aes(x=n, reorder(splitted, n))) +
  geom_bar(stat="identity", fill=hcl(195,100,65)) +
  ggtitle("Most frequent words in spam messages") +
  ylab("Word") +
  xlab("Word count") +
  theme(plot.title = element_text(hjust = 0.5))

ggplot( most_frequent_ham, aes(x=n, reorder(splitted, n))) +
  geom_bar(stat="identity", fill=hcl(195,100,65)) +
  ggtitle("Most frequent words in ham messages") +
  ylab("Word") +
  xlab("Word count") +
  theme(plot.title = element_text(hjust = 0.5))

```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",

   fields = c("bag", "y", "length", "spam_length", "ham_length", "prediction_df", "precision", "recall"),
   
   methods = list(

    fit = function(X, y) {
        bag <<- X
        y <<- y
        length <<- sum(rowSums(bag))
        spam_length <<- sum(rowSums(bag[, y==1]))
        ham_length <<- sum(rowSums(bag[, y==0]))
    },
    
    predict = function(message) {
      words <- which(message >= 1)
      
      if (length(words) > 1) {
        reduced_spam_bag = rowSums(bag[words, y==1]) + 1
        reduced_ham_bag = rowSums(bag[words, y==0]) + 1
      } else {
        reduced_spam_bag = sum(bag[words, y==1]) + 1
        reduced_ham_bag = sum(bag[words, y==0]) + 1
      }
      
      spam_class_probability <- spam_length / length
      ham_class_probability <- ham_length / length
      
      spam_observation_probability <- prod(reduced_spam_bag / spam_length)
      ham_observation_probability <- prod(reduced_ham_bag / ham_length)

      observation_probability <- spam_class_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
      
        
      spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
      ham_probability <- ham_class_probability * ham_observation_probability / observation_probability

      
      if (spam_probability > ham_probability)
        return(1)
      return(0)
    },
      
    score = function(X_test, y_test){
      
      prediction_df_build <- data.frame (
        TP = numeric(),
        TN = numeric(),
        FP = numeric(),
        FN = numeric()
      )
      
      for (i in 1:length(y_test)) {
        
        true_category <- strtoi(y_test[i])
        category <- predict(X_test[,i])
        
        new_row <- data.frame(TP = 0, TN = 0, FP = 0, FN = 0)
        
        if (category == 1 & true_category == 1) {
          new_row$TP <- 1
        } else if (category == 0 & true_category == 0) {
          new_row$TN <- 1
        } else if (category == 1 & true_category == 0) {
          new_row$FP <- 1
        } else if (category == 0 & true_category == 1) {
          new_row$FN <- 1
        }
        
        prediction_df_build <- rbind(prediction_df_build, new_row)
      }
      
      prediction_df <<- prediction_df_build

      true_positive <- sum(prediction_df$TP)
      true_negative <- sum(prediction_df$TN)
      false_positive <- sum(prediction_df$FP)
      false_negative <- sum(prediction_df$FN)
      
      precision <<- true_positive / (true_positive + false_positive)
      recall <<- true_positive / (true_positive + false_negative)    
      
      return( 2*precision*recall / (precision + recall));
    }
))

clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stopwords("en"))
    return(corpus)
}

data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
dat_corpus <- clean_corpus(dat_corpus)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
vocab <- Terms(data_tdm)

test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_corpus <- clean_corpus(test_corpus)
test_tdm <- TermDocumentMatrix(test_corpus, control = list(dictionary = vocab))
test_bag_words <- as.matrix(test_tdm)


train[train=="spam"] <- 1
train[train=="ham"] <- 0

test[test=="spam"] <- 1
test[test=="ham"] <- 0


model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$score(X_test=test_bag_words, y_test=test$Category)
```

## Measure effectiveness of your classifier


## Precision, Recall and F1 chart
```{r}
confusion_df <- model$prediction_df


true_positive <- sum(confusion_df$TP)
true_negative <- sum(confusion_df$TN)
false_positive <- sum(confusion_df$FP)
false_negative <- sum(confusion_df$FN)


confusion_summary <- data.frame(
  Predicted = c("Spam", "Spam", "Ham", "Ham"),
  Actual = c("Spam", "Ham", "Spam", "Ham"),
  Count = c(true_positive, false_positive, false_negative, true_negative)
)


total_predictions <- sum(confusion_summary$Count)
confusion_summary$Percentage <- (confusion_summary$Count / total_predictions) * 100
confusion_summary$Label <- paste("Predicted:", confusion_summary$Predicted,
                                  "\nActual:", confusion_summary$Actual,
                                  "\n", round(confusion_summary$Percentage, 1), "%")


library(ggplot2)

ggplot(confusion_summary, aes(x = "", y = Count, fill = Label)) +
  geom_bar(stat = "identity") +
  coord_polar("y", start = 0) +
  theme_minimal() +
  labs(title = "Confusion Matrix Chart") +
  scale_fill_manual(values = c("green", "blue", "yellow", "red")) +
  theme(
    plot.title = element_text(hjust = 0.5, family = "sans", size = 18, face = "bold"),
    axis.title.x = element_blank(), 
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    legend.title = element_blank(),
    legend.text = element_text(size = 12, family = "sans"),
    legend.spacing.y = unit(1, "cm")
  )
```

```{r}

predictions <- model$prediction_df$TP + 2 * model$prediction_df$FP
true_labels <- test$Category

false_positives <- test[model$prediction_df$FP == 1, ]
false_negatives <- test[model$prediction_df$FN == 1, ]

cat("False Positive Messages:\n\n")
print(false_positives$Message)

cat("\nFalse Negative Messages:\n\n")
print(false_negatives$Message)
```


## Conclusions

## Conclusions

fit(X, y):

This method trains the classifier using the provided training data. It takes in the feature
matrix X (bag-of-words) and the class labels y. It calculates the amount of occurring of
each class, storing these values for use during predict function.

predict(message):

The predict method classifies a new message based on the learned probabilities. It 
transforms the message into its bag-of-words representation and computes the posterior 
probabilities for each class. The class with the highest probability is returned as the 
predicted label. It uses total probability rule and Bayes formula in order to compute
probabilities of spam or ham given the observations.

score(X_test, y_test):

This method evaluates the classifier's performance on a test set. It takes the test feature
matrix and the true class labels, calculating true positives, true negatives, false 
positives, and false negatives. It then computes the precision, recall, and F1 score, 
providing a comprehensive assessment of the classifier's effectiveness.

-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.

Pros:

Simplicity and Efficiency: The Naive Bayes classifier is efficient, making it suitable for 
large data sets. The training and prediction processes are fast and simple, often 
requiring just a few lines of code.

Effective for Text Classification: Despite its simplicity, the classifier often performs 
surprisingly well for text classification tasks, especially with a large data set, as 
demonstrated in our assignment.

Works with Small Data sets: Unlike many other classifiers, Naive Bayes can produce good 
results even with relatively small training data sets.

Cons:

Independence Assumption: The assumption that features are independent is often unrealistic.
In practice, features (words) can be correlated, which may lead to sub optimal 
classification performance.

Zero Probability Problem: If a word appears in the test data but not in the training data, 
it may lead to a zero probability estimation, making the entire classification unreliable. 
This is typically addressed through techniques like Laplace smoothing.

Binary Classification Limitation: While effective for binary classification (spam vs. ham),
it may struggle with problems that need multiple classes unless carefully managed.

-   Explain why accuracy is not a good choice for the base metrics for
    classification tasks. Why F1 score is always preferable?
    
Accuracy is often not a reliable metric for evaluating classification models, particularly 
in cases of class imbalance. In our assignment, for instance, if 90% of messages are ham 
and only 10% are spam, a naive classifier that always predicts ham will achieve 90% 
accuracy without actually being useful. Thus, it is crucial to consider other metrics that 
provide more insight into the model’s performance on each class.

The F1 score eliminates this problem. It is the harmonic mean of precision and recall, 
making it a more balanced measure than accuracy, especially when dealing with imbalanced 
classes. It accounts for both false positives and false negatives, providing a single score
that reflects the model’s ability to correctly classify positive instances while minimizing
misclassifications.



