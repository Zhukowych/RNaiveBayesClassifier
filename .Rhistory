bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- unlist(which(message == 1))
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
spam_class_probability <- spam_length / length
ham_class_probability <- ham_length / length
observation_probability <- spam_observation_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
ham_probability <- ham_class_probability * ham_observation_probability / observation_probability
print(spam_probability)
print(ham_probability)
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test){
true_positive <- 0
true_negative <- 0
false_positive <- 0
false_negative <- 0
for (i in 1:length(X_test)) {
true_category <- y_test[i]
category <- predict(X_test[i]);
if (category == "spam" & true_category == "spam")
true_positive <- true_positive + 1
else if (category == "ham" && true_category == "ham")
true_negative <- true_negative + 1
else if (category == "spam" && true_category == "ham")
false_positive <- false_positive + 1
else if (category == "ham" && true_category == "spam")
false_negative <- false_negative + 1
}
precision <- true_positive / (true_positive + false_positive)
recall <- true_positive / (true_positive + false_negative)
return( 2*precision*recall / (precision + recall));
}
))
data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_tdm <- TermDocumentMatrix(test_corpus)
test_bag_words <- as.matrix(data_tdm)
train[train=="spam"] <- 1
train[train=="ham"] <- 0
model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$predict(unlist(test_bag_words[,1]))
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = c("bag", "y", "length", "spam_length", "ham_length"),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- unlist(which(message == 1))
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
spam_class_probability <- spam_length / length
ham_class_probability <- ham_length / length
observation_probability <- spam_observation_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
ham_probability <- ham_class_probability * ham_observation_probability / observation_probability
if (spam_probability > ham_probability)
return("spam")
return("ham")
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test){
true_positive <- 0
true_negative <- 0
false_positive <- 0
false_negative <- 0
for (i in 1:length(X_test)) {
true_category <- y_test[i]
category <- predict(X_test[i]);
if (category == "spam" & true_category == "spam")
true_positive <- true_positive + 1
else if (category == "ham" && true_category == "ham")
true_negative <- true_negative + 1
else if (category == "spam" && true_category == "ham")
false_positive <- false_positive + 1
else if (category == "ham" && true_category == "spam")
false_negative <- false_negative + 1
}
precision <- true_positive / (true_positive + false_positive)
recall <- true_positive / (true_positive + false_negative)
return( 2*precision*recall / (precision + recall));
}
))
data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_tdm <- TermDocumentMatrix(test_corpus)
test_bag_words <- as.matrix(data_tdm)
train[train=="spam"] <- 1
train[train=="ham"] <- 0
model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$predict(unlist(test_bag_words[,1]))
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = c("bag", "y", "length", "spam_length", "ham_length"),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- unlist(which(message == 1))
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
spam_class_probability <- spam_length / length
ham_class_probability <- ham_length / length
observation_probability <- spam_observation_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
ham_probability <- ham_class_probability * ham_observation_probability / observation_probability
if (spam_probability > ham_probability)
return(1)
return(0)
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test){
true_positive <- 0
true_negative <- 0
false_positive <- 0
false_negative <- 0
for (i in 1:length(X_test)) {
true_category <- y_test[i]
category <- predict(X_test[i]);
if (category == "spam" & true_category == "spam")
true_positive <- true_positive + 1
else if (category == "ham" && true_category == "ham")
true_negative <- true_negative + 1
else if (category == "spam" && true_category == "ham")
false_positive <- false_positive + 1
else if (category == "ham" && true_category == "spam")
false_negative <- false_negative + 1
}
precision <- true_positive / (true_positive + false_positive)
recall <- true_positive / (true_positive + false_negative)
return( 2*precision*recall / (precision + recall));
}
))
data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_tdm <- TermDocumentMatrix(test_corpus)
test_bag_words <- as.matrix(data_tdm)
train[train=="spam"] <- 1
train[train=="ham"] <- 0
model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$score(X_test=test_bag_words, y=test$Category)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = c("bag", "y", "length", "spam_length", "ham_length"),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- unlist(which(message == 1))
print(words)
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
spam_class_probability <- spam_length / length
ham_class_probability <- ham_length / length
observation_probability <- spam_observation_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
ham_probability <- ham_class_probability * ham_observation_probability / observation_probability
if (spam_probability > ham_probability)
return(1)
return(0)
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test){
true_positive <- 0
true_negative <- 0
false_positive <- 0
false_negative <- 0
for (i in 1:length(X_test)) {
true_category <- y_test[i]
category <- predict(X_test[i]);
if (category == "spam" & true_category == "spam")
true_positive <- true_positive + 1
else if (category == "ham" && true_category == "ham")
true_negative <- true_negative + 1
else if (category == "spam" && true_category == "ham")
false_positive <- false_positive + 1
else if (category == "ham" && true_category == "spam")
false_negative <- false_negative + 1
}
precision <- true_positive / (true_positive + false_positive)
recall <- true_positive / (true_positive + false_negative)
return( 2*precision*recall / (precision + recall));
}
))
data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_tdm <- TermDocumentMatrix(test_corpus)
test_bag_words <- as.matrix(data_tdm)
train[train=="spam"] <- 1
train[train=="ham"] <- 0
model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$score(X_test=test_bag_words, y=test$Category)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = c("bag", "y", "length", "spam_length", "ham_length"),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- which(message == 1)
print(words)
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
spam_class_probability <- spam_length / length
ham_class_probability <- ham_length / length
observation_probability <- spam_observation_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
ham_probability <- ham_class_probability * ham_observation_probability / observation_probability
if (spam_probability > ham_probability)
return(1)
return(0)
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test){
true_positive <- 0
true_negative <- 0
false_positive <- 0
false_negative <- 0
for (i in 1:length(X_test)) {
true_category <- y_test[i]
category <- predict(X_test[i]);
if (category == "spam" & true_category == "spam")
true_positive <- true_positive + 1
else if (category == "ham" && true_category == "ham")
true_negative <- true_negative + 1
else if (category == "spam" && true_category == "ham")
false_positive <- false_positive + 1
else if (category == "ham" && true_category == "spam")
false_negative <- false_negative + 1
}
precision <- true_positive / (true_positive + false_positive)
recall <- true_positive / (true_positive + false_negative)
return( 2*precision*recall / (precision + recall));
}
))
data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_tdm <- TermDocumentMatrix(test_corpus)
test_bag_words <- as.matrix(data_tdm)
train[train=="spam"] <- 1
train[train=="ham"] <- 0
model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$score(X_test=test_bag_words, y=test$Category)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = c("bag", "y", "length", "spam_length", "ham_length"),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- which(message == 1)
print(words)
if (length(words) > 1) {
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
} else {
spam_observation_probability <- prod(sum(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(sum(bag[words,y==0]) / ham_length)
}
spam_class_probability <- spam_length / length
ham_class_probability <- ham_length / length
observation_probability <- spam_observation_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
ham_probability <- ham_class_probability * ham_observation_probability / observation_probability
if (spam_probability > ham_probability)
return(1)
return(0)
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test){
true_positive <- 0
true_negative <- 0
false_positive <- 0
false_negative <- 0
for (i in 1:length(X_test)) {
true_category <- y_test[i]
category <- predict(X_test[i]);
if (category == "spam" & true_category == "spam")
true_positive <- true_positive + 1
else if (category == "ham" && true_category == "ham")
true_negative <- true_negative + 1
else if (category == "spam" && true_category == "ham")
false_positive <- false_positive + 1
else if (category == "ham" && true_category == "spam")
false_negative <- false_negative + 1
}
precision <- true_positive / (true_positive + false_positive)
recall <- true_positive / (true_positive + false_negative)
return( 2*precision*recall / (precision + recall));
}
))
data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_tdm <- TermDocumentMatrix(test_corpus)
test_bag_words <- as.matrix(data_tdm)
train[train=="spam"] <- 1
train[train=="ham"] <- 0
model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$score(X_test=test_bag_words, y=test$Category)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = c("bag", "y", "length", "spam_length", "ham_length"),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- which(message == 1)
print(words)
if (length(words) > 1) {
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
} else (length(words) == 1){
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of words!
fields = c("bag", "y", "length", "spam_length", "ham_length"),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y) {
bag <<- X
y <<- y
length <<- sum(rowSums(bag))
spam_length <<- sum(rowSums(bag[, y==1]))
ham_length <<- sum(rowSums(bag[, y==0]))
},
# return prediction for a single message
predict = function(message) {
words <- which(message == 1)
print(words)
if (length(words) > 1) {
spam_observation_probability <- prod(rowSums(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(rowSums(bag[words,y==0]) / ham_length)
} else if (length(words) == 1){
spam_observation_probability <- prod(sum(bag[words,y==1]) / spam_length)
ham_observation_probability <- prod(sum(bag[words,y==0]) / ham_length)
} else {
return(1);
}
spam_class_probability <- spam_length / length
ham_class_probability <- ham_length / length
observation_probability <- spam_observation_probability * spam_observation_probability + ham_class_probability * ham_observation_probability
spam_probability <- spam_class_probability * spam_observation_probability / observation_probability
ham_probability <- ham_class_probability * ham_observation_probability / observation_probability
if (spam_probability > ham_probability)
return(1)
return(0)
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test){
true_positive <- 0
true_negative <- 0
false_positive <- 0
false_negative <- 0
for (i in 1:length(X_test)) {
true_category <- y_test[i]
category <- predict(X_test[i]);
if (category == "spam" & true_category == "spam")
true_positive <- true_positive + 1
else if (category == "ham" && true_category == "ham")
true_negative <- true_negative + 1
else if (category == "spam" && true_category == "ham")
false_positive <- false_positive + 1
else if (category == "ham" && true_category == "spam")
false_negative <- false_negative + 1
}
precision <- true_positive / (true_positive + false_positive)
recall <- true_positive / (true_positive + false_negative)
return( 2*precision*recall / (precision + recall));
}
))
data_source <- VectorSource(train$Message)
dat_corpus <- VCorpus(data_source)
data_tdm <- TermDocumentMatrix(dat_corpus)
bag_of_words <- as.matrix(data_tdm)
test_source <- VectorSource(test$Message)
test_corpus <- VCorpus(test_source)
test_tdm <- TermDocumentMatrix(test_corpus)
test_bag_words <- as.matrix(data_tdm)
train[train=="spam"] <- 1
train[train=="ham"] <- 0
model = naiveBayes()
model$fit(X=bag_of_words, y=train$Category)
model$score(X_test=test_bag_words, y=test$Category)
